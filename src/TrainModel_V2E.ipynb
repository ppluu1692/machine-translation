{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QbuRIDbu8X-"
      },
      "source": [
        "## Cài đặt các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp5ZVwgS2lYi",
        "outputId": "85257bbb-0456-4464-bb50-a4c632c542ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Collecting https://gitlab.com/trungtv/vi_spacy/-/raw/master/vi_core_news_lg/dist/vi_core_news_lg-0.0.1.tar.gz\n",
            "  Downloading https://gitlab.com/trungtv/vi_spacy/-/raw/master/vi_core_news_lg/dist/vi_core_news_lg-0.0.1.tar.gz (254.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.1.0,>=3.0.5 (from vi-core-news-lg==0.0.1)\n",
            "  Downloading spacy-3.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (3.0.8)\n",
            "Collecting thinc<8.1.0,>=8.0.3 (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (0.7.9)\n",
            "Collecting wasabi<1.1.0,>=0.8.1 (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (2.0.8)\n",
            "Collecting typer<0.4.0,>=0.3.0 (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1)\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (2.27.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (3.4)\n",
            "Collecting click<7.2.0,>=7.1.1 (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1)\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.1.0,>=3.0.5->vi-core-news-lg==0.0.1) (2.1.3)\n",
            "Building wheels for collected packages: vi-core-news-lg\n",
            "  Building wheel for vi-core-news-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vi-core-news-lg: filename=vi_core_news_lg-0.0.1-py3-none-any.whl size=254513599 sha256=3accc2124fe39ae912089730eac063ecfc903ee915a3ecbe702ee6c583feceb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/44/ad/99e9a242b60be7acebe53f65724bbb52efc0e8d6449e5c7552\n",
            "Successfully built vi-core-news-lg\n",
            "Installing collected packages: wasabi, pydantic, click, typer, thinc, spacy, vi-core-news-lg\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.9\n",
            "    Uninstalling pydantic-1.10.9:\n",
            "      Successfully uninstalled pydantic-1.10.9\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.10\n",
            "    Uninstalling thinc-8.1.10:\n",
            "      Successfully uninstalled thinc-8.1.10\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.3\n",
            "    Uninstalling spacy-3.5.3:\n",
            "      Successfully uninstalled spacy-3.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fiona 1.9.4.post1 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.0.9 which is incompatible.\n",
            "inflect 6.0.4 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-7.1.2 pydantic-1.8.2 spacy-3.0.9 thinc-8.0.17 typer-0.3.2 vi-core-news-lg-0.0.1 wasabi-0.10.1\n",
            "Collecting Pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from Pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from Pyvi)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->Pyvi) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->Pyvi) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->Pyvi) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->Pyvi) (3.1.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->Pyvi)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->Pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->Pyvi) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->Pyvi) (4.65.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, Pyvi\n",
            "Successfully installed Pyvi-0.1.1 python-crfsuite-0.9.9 sklearn-crfsuite-0.3.6\n",
            "Requirement already satisfied: en_core_web_sm in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Collecting spacy<3.6.0,>=3.5.0 (from en_core_web_sm)\n",
            "  Downloading spacy-3.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.0.8)\n",
            "Collecting thinc<8.2.0,>=8.1.8 (from spacy<3.6.0,>=3.5.0->en_core_web_sm)\n",
            "  Downloading thinc-8.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (913 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.3/913.3 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.3.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en_core_web_sm) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.1.3)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.0.9\n",
            "    Uninstalling spacy-3.0.9:\n",
            "      Successfully uninstalled spacy-3.0.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vi-core-news-lg 0.0.1 requires spacy<3.1.0,>=3.0.5, but you have spacy 3.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.5.4 thinc-8.1.10\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=17109c80109e1b5e728a9d8907930d6f460960bfa877c71d51e7e62fa84adc10\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.6.0\n",
        "!pip install spacy\n",
        "!pip install https://gitlab.com/trungtv/vi_spacy/-/raw/master/vi_core_news_lg/dist/vi_core_news_lg-0.0.1.tar.gz\n",
        "!pip install Pyvi\n",
        "!pip install en_core_web_sm\n",
        "!pip install langdetect\n",
        "!pip install contractions\n",
        "!pip install dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1oTAZIfw3DCc"
      },
      "outputs": [],
      "source": [
        "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
        "import os\n",
        "import langdetect\n",
        "import re\n",
        "import torch\n",
        "import torch.optim as  optim\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "import spacy\n",
        "from spacy.lang.vi import Vietnamese\n",
        "import contractions\n",
        "import pandas as pd\n",
        "import dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1C4pZsGJ3KaV"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7qC4zEvOrJ"
      },
      "source": [
        "## Chuẩn bị dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMWsgkVwv0B7"
      },
      "source": [
        "Mô hình token hóa dữ liệu tiếng Anh và tiếng Việt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VI8ocYwy31gh"
      },
      "outputs": [],
      "source": [
        "nlp_vi = Vietnamese()\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize_vi(text):\n",
        "    return [token.text for token in nlp_vi.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [token.text for token in nlp_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6bnKv72L32JJ"
      },
      "outputs": [],
      "source": [
        "source = Field(tokenize=tokenize_vi, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "target = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "\n",
        "fields = {\"Vietnamese\": (\"src\", source), \"English\": (\"trg\", target)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp0Aurs2wCvj"
      },
      "source": [
        "Đọc và xử lý dữ liệu:\n",
        "- Loại bỏ các ký tự không phải từ, xóa dấu cách dư thừa.\n",
        "- Đối với tiếng Anh: Chuyển đổi các từ viết gọn (Vd: I'm -> I am, you're -> you are)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Ig7R7w5CQE",
        "outputId": "bef965a7-8931-4373-87ea-0f6834063827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-29 15:26:37--  https://github.com/ppl160902/statistical_learning/raw/main/dataset/english_vietnamese.csv\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ppl160902/statistical_learning/main/dataset/english_vietnamese.csv [following]\n",
            "--2023-06-29 15:26:37--  https://raw.githubusercontent.com/ppl160902/statistical_learning/main/dataset/english_vietnamese.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21186474 (20M) [text/plain]\n",
            "Saving to: ‘english_vietnamese.csv’\n",
            "\n",
            "english_vietnamese. 100%[===================>]  20.20M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-06-29 15:26:38 (150 MB/s) - ‘english_vietnamese.csv’ saved [21186474/21186474]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://github.com/ppl160902/statistical_learning/raw/main/dataset/english_vietnamese.csv\"\n",
        "df = pd.read_csv(\"/content/english_vietnamese.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hHgoEeea5K1i"
      },
      "outputs": [],
      "source": [
        "def transform_text(text, eng=True):\n",
        "    if eng:\n",
        "        text = contractions.fix(text)\n",
        "    s = re.sub(r'[^\\w\\s]', '', text)\n",
        "    s = re.sub('\\s+',' ',s).strip()\n",
        "    return s.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2WIGCUp75VoX"
      },
      "outputs": [],
      "source": [
        "df['english'] = df['english'].apply(transform_text)\n",
        "df['vietnamese'] = df['vietnamese'].apply(transform_text, eng=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ugOPFAUY347D"
      },
      "outputs": [],
      "source": [
        "sources = df['vietnamese'].to_list()\n",
        "targets = df['english'].to_list()\n",
        "examples = []\n",
        "for src_sentence, trg_sentence in zip(sources, targets):\n",
        "    example = Example.fromlist([src_sentence, trg_sentence], fields=[('src', source), ('trg', target)])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw6-rv7_wywR"
      },
      "source": [
        "Tạo dateset, gồm train data, valid data và test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_G4ezWk47_y",
        "outputId": "9df603fc-1d02-44c5-c6a3-2694b2638663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('i', 54511), ('the', 45249), ('to', 45199), ('tom', 37461), ('you', 37283), ('is', 30458), ('not', 29919), ('a', 28317), ('do', 17117), ('he', 16689)]\n"
          ]
        }
      ],
      "source": [
        "# Tạo dataset từ example\n",
        "dataset = Dataset(examples, fields=[('src', source), ('trg', target)])\n",
        "\n",
        "# Chia train, valid, test\n",
        "train_data, valid_data, test_data = dataset.split(split_ratio=[0.7, 0.15, 0.15])\n",
        "\n",
        "# Xây dựng vocabulary\n",
        "source.build_vocab(train_data,min_freq=2)\n",
        "target.build_vocab(train_data,min_freq=2)\n",
        "\n",
        "print(target.vocab.freqs.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A0pwMSC2w8xS"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/source_V2E.Field\",\"wb\")as f:\n",
        "     dill.dump(source,f)\n",
        "\n",
        "with open(\"/content/target_V2E.Field\",\"wb\")as f:\n",
        "     dill.dump(target,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXkS4i5jvXIk"
      },
      "source": [
        "## Thiết lập và chọn tham số mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RgDURW1rA5g7"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator,valid_iterator,test_iterator = data.BucketIterator.splits(\n",
        "    (train_data,valid_data,test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort=False,\n",
        "    sort_within_batch=False,\n",
        "shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gPmCdn_p6c8A"
      },
      "outputs": [],
      "source": [
        "class TranslateTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        max_len,\n",
        "    ):\n",
        "        super(TranslateTransformer, self).__init__()\n",
        "        self.srcEmbeddings = nn.Embedding(src_vocab_size,embedding_size)\n",
        "        self.trgEmbeddings= nn.Embedding(trg_vocab_size,embedding_size)\n",
        "        self.srcPositionalEmbeddings= nn.Embedding(max_len,embedding_size)\n",
        "        self.trgPositionalEmbeddings= nn.Embedding(max_len,embedding_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src.transpose(0,1) == self.src_pad_idx\n",
        "\n",
        "        return src_mask.to(device)\n",
        "\n",
        "    def forward(self,x,trg):\n",
        "        src_seq_length = x.shape[0]\n",
        "        N = x.shape[1]\n",
        "        trg_seq_length = trg.shape[0]\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length)\n",
        "            .reshape(src_seq_length,1)  + torch.zeros(src_seq_length,N)\n",
        "        ).to(device)\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length)\n",
        "            .reshape(trg_seq_length,1)  + torch.zeros(trg_seq_length,N)\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "        srcWords = self.dropout(self.srcEmbeddings(x.long()) +self.srcPositionalEmbeddings(src_positions.long()))\n",
        "        trgWords = self.dropout(self.trgEmbeddings(trg.long())+self.trgPositionalEmbeddings(trg_positions.long()))\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(x)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(device)\n",
        "\n",
        "\n",
        "        out = self.transformer(srcWords,trgWords, src_key_padding_mask=src_padding_mask,tgt_mask=trg_mask )\n",
        "        out= self.fc_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDoGT1NwA70d",
        "outputId": "9c459153-8df4-4e99-bd55-8084dab7c99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vietnamese vocabulary: 7878\n",
            "Size of English vocabulary: 11721\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size  = len(source.vocab)\n",
        "print(\"Size of Vietnamese vocabulary:\",src_vocab_size)\n",
        "\n",
        "trg_vocab_size =len(target.vocab)\n",
        "print(\"Size of English vocabulary:\",trg_vocab_size)\n",
        "\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "\n",
        "max_len= 225\n",
        "embedding_size= 256\n",
        "src_pad_idx = source.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "model = TranslateTransformer(\n",
        "    embedding_size,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    max_len\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN1yYQIUviHR"
      },
      "source": [
        "## Huấn luyện mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Gbl49UpZA-FW"
      },
      "outputs": [],
      "source": [
        "loss_track = []\n",
        "loss_validation_track= []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWprWeENBAlf",
        "outputId": "65886af6-1a41-47e4-befb-5ea031138145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train crossentropy at epoch 0 loss:  3.5509461647314993\n",
            "validation crossentropy at epoch 0 loss:  2.6112732007199484\n",
            "Accuracy: 0.2701459813327222\n",
            "train crossentropy at epoch 1 loss:  2.45456999128671\n",
            "validation crossentropy at epoch 1 loss:  2.0464280997746744\n",
            "Accuracy: 0.3043949756806131\n",
            "train crossentropy at epoch 2 loss:  2.0751578341713914\n",
            "validation crossentropy at epoch 2 loss:  1.7808274150694776\n",
            "Accuracy: 0.32182348280292017\n",
            "train crossentropy at epoch 3 loss:  1.8538631701212136\n",
            "validation crossentropy at epoch 3 loss:  1.6286056319739195\n",
            "Accuracy: 0.3321198241311886\n",
            "train crossentropy at epoch 4 loss:  1.7022594409773675\n",
            "validation crossentropy at epoch 4 loss:  1.5083056906325705\n",
            "Accuracy: 0.34033200863370666\n",
            "train crossentropy at epoch 5 loss:  1.5889746635080242\n",
            "validation crossentropy at epoch 5 loss:  1.4380361943836981\n",
            "Accuracy: 0.34310788197947684\n",
            "train crossentropy at epoch 6 loss:  1.5028387128877982\n",
            "validation crossentropy at epoch 6 loss:  1.361354123225948\n",
            "Accuracy: 0.34818254337872173\n",
            "train crossentropy at epoch 7 loss:  1.4297552564161287\n",
            "validation crossentropy at epoch 7 loss:  1.3096671228440815\n",
            "Accuracy: 0.35388157528269304\n",
            "train crossentropy at epoch 8 loss:  1.370963044046498\n",
            "validation crossentropy at epoch 8 loss:  1.2758639906876839\n",
            "Accuracy: 0.35502679899135114\n",
            "train crossentropy at epoch 9 loss:  1.3203521740736721\n",
            "validation crossentropy at epoch 9 loss:  1.225863045194005\n",
            "Accuracy: 0.36094459582198\n",
            "train crossentropy at epoch 10 loss:  1.2741464865079029\n",
            "validation crossentropy at epoch 10 loss:  1.2049604376290468\n",
            "Accuracy: 0.3604176649714807\n",
            "train crossentropy at epoch 11 loss:  1.2333431862669884\n",
            "validation crossentropy at epoch 11 loss:  1.1705112040242893\n",
            "Accuracy: 0.36223832142873685\n",
            "train crossentropy at epoch 12 loss:  1.200404875441421\n",
            "validation crossentropy at epoch 12 loss:  1.178046452819101\n",
            "Accuracy: 0.3624011979745585\n",
            "train crossentropy at epoch 13 loss:  1.1672433398181585\n",
            "validation crossentropy at epoch 13 loss:  1.122746052057951\n",
            "Accuracy: 0.36470517563775157\n",
            "train crossentropy at epoch 14 loss:  1.1380280565014846\n",
            "validation crossentropy at epoch 14 loss:  1.0995610162315752\n",
            "Accuracy: 0.3667760365704973\n",
            "train crossentropy at epoch 15 loss:  1.1107174990846098\n",
            "validation crossentropy at epoch 15 loss:  1.087321477048349\n",
            "Accuracy: 0.3693491896045072\n",
            "train crossentropy at epoch 16 loss:  1.0872891950307133\n",
            "validation crossentropy at epoch 16 loss:  1.0651675034689423\n",
            "Accuracy: 0.36899926287122453\n",
            "train crossentropy at epoch 17 loss:  1.0651068214246695\n",
            "validation crossentropy at epoch 17 loss:  1.049722338282822\n",
            "Accuracy: 0.37056259018341475\n",
            "train crossentropy at epoch 18 loss:  1.04367974428393\n",
            "validation crossentropy at epoch 18 loss:  1.0384324112994558\n",
            "Accuracy: 0.3704722639603912\n",
            "train crossentropy at epoch 19 loss:  1.0240895670523746\n",
            "validation crossentropy at epoch 19 loss:  1.0257865071496708\n",
            "Accuracy: 0.37168739150677244\n",
            "train crossentropy at epoch 20 loss:  1.006287557915818\n",
            "validation crossentropy at epoch 20 loss:  1.011569689784274\n",
            "Accuracy: 0.3728729214523527\n",
            "train crossentropy at epoch 21 loss:  0.9891564586822935\n",
            "validation crossentropy at epoch 21 loss:  1.0116689602200617\n",
            "Accuracy: 0.37420755119783916\n",
            "train crossentropy at epoch 22 loss:  0.973000854382412\n",
            "validation crossentropy at epoch 22 loss:  0.991998484070669\n",
            "Accuracy: 0.37569353034983166\n",
            "train crossentropy at epoch 23 loss:  0.9573839602871336\n",
            "validation crossentropy at epoch 23 loss:  0.9818736753807772\n",
            "Accuracy: 0.3750566390175789\n",
            "train crossentropy at epoch 24 loss:  0.9428891553938817\n",
            "validation crossentropy at epoch 24 loss:  0.9751902557659469\n",
            "Accuracy: 0.3741828409652686\n",
            "train crossentropy at epoch 25 loss:  0.9292706067399156\n",
            "validation crossentropy at epoch 25 loss:  0.9651465271943368\n",
            "Accuracy: 0.37799020093654767\n",
            "train crossentropy at epoch 26 loss:  0.9161914400702758\n",
            "validation crossentropy at epoch 26 loss:  0.958010009971241\n",
            "Accuracy: 0.3755005258981787\n",
            "train crossentropy at epoch 27 loss:  0.9042291368940751\n",
            "validation crossentropy at epoch 27 loss:  0.9515663195376428\n",
            "Accuracy: 0.3787812702036372\n",
            "train crossentropy at epoch 28 loss:  0.8926637487445804\n",
            "validation crossentropy at epoch 28 loss:  0.9452942929811926\n",
            "Accuracy: 0.37943683762667285\n",
            "train crossentropy at epoch 29 loss:  0.8815895487882679\n",
            "validation crossentropy at epoch 29 loss:  0.9337434780677693\n",
            "Accuracy: 0.3783983157998488\n",
            "train crossentropy at epoch 30 loss:  0.8712907709663721\n",
            "validation crossentropy at epoch 30 loss:  0.9343645428651132\n",
            "Accuracy: 0.38021370071052774\n",
            "train crossentropy at epoch 31 loss:  0.8605118625884434\n",
            "validation crossentropy at epoch 31 loss:  0.926184879553398\n",
            "Accuracy: 0.3786841546336854\n",
            "train crossentropy at epoch 32 loss:  0.8509953564019512\n",
            "validation crossentropy at epoch 32 loss:  0.9208515183437591\n",
            "Accuracy: 0.38031985941874347\n",
            "train crossentropy at epoch 33 loss:  0.8405407511930671\n",
            "validation crossentropy at epoch 33 loss:  0.9130414249312958\n",
            "Accuracy: 0.3812957970893663\n",
            "train crossentropy at epoch 34 loss:  0.8312830987594111\n",
            "validation crossentropy at epoch 34 loss:  0.9120024368666962\n",
            "Accuracy: 0.38129021995790174\n",
            "train crossentropy at epoch 35 loss:  0.8240861452097515\n",
            "validation crossentropy at epoch 35 loss:  0.9011913645987543\n",
            "Accuracy: 0.38190188311066786\n",
            "train crossentropy at epoch 36 loss:  0.8159869503417461\n",
            "validation crossentropy at epoch 36 loss:  0.904143282931123\n",
            "Accuracy: 0.3821849347804903\n",
            "train crossentropy at epoch 37 loss:  0.8077498824047528\n",
            "validation crossentropy at epoch 37 loss:  0.8919834856618971\n",
            "Accuracy: 0.38236551044242817\n",
            "train crossentropy at epoch 38 loss:  0.7980726237830927\n",
            "validation crossentropy at epoch 38 loss:  0.889811207384071\n",
            "Accuracy: 0.3810270144852712\n",
            "train crossentropy at epoch 39 loss:  0.7912713386600824\n",
            "validation crossentropy at epoch 39 loss:  0.883741325380018\n",
            "Accuracy: 0.3815129563622894\n",
            "train crossentropy at epoch 40 loss:  0.7836919890140458\n",
            "validation crossentropy at epoch 40 loss:  0.9039142896865038\n",
            "Accuracy: 0.38131643197119536\n",
            "train crossentropy at epoch 41 loss:  0.7769882215763168\n",
            "validation crossentropy at epoch 41 loss:  0.8754798307914862\n",
            "Accuracy: 0.3825444183037109\n",
            "train crossentropy at epoch 42 loss:  0.7704298956359891\n",
            "validation crossentropy at epoch 42 loss:  0.8763472971900198\n",
            "Accuracy: 0.3846658253532279\n",
            "train crossentropy at epoch 43 loss:  0.7625766441869221\n",
            "validation crossentropy at epoch 43 loss:  0.8683739191533735\n",
            "Accuracy: 0.38503046692240017\n",
            "train crossentropy at epoch 44 loss:  0.7570565235432533\n",
            "validation crossentropy at epoch 44 loss:  0.865674396289275\n",
            "Accuracy: 0.38393561549271465\n",
            "train crossentropy at epoch 45 loss:  0.749546027151372\n",
            "validation crossentropy at epoch 45 loss:  0.8639992950546661\n",
            "Accuracy: 0.3855789242563403\n",
            "train crossentropy at epoch 46 loss:  0.7442721350587529\n",
            "validation crossentropy at epoch 46 loss:  0.8605844844507691\n",
            "Accuracy: 0.38617830806095094\n",
            "train crossentropy at epoch 47 loss:  0.7387786186856331\n",
            "validation crossentropy at epoch 47 loss:  0.8571559973311105\n",
            "Accuracy: 0.38672242372784194\n",
            "train crossentropy at epoch 48 loss:  0.7320253516808688\n",
            "validation crossentropy at epoch 48 loss:  0.8541587417157704\n",
            "Accuracy: 0.38696425723394995\n",
            "train crossentropy at epoch 49 loss:  0.7263873202954051\n",
            "validation crossentropy at epoch 49 loss:  0.8500179618397015\n",
            "Accuracy: 0.38380274665492176\n",
            "train crossentropy at epoch 50 loss:  0.7212293316456053\n",
            "validation crossentropy at epoch 50 loss:  0.8492860907576228\n",
            "Accuracy: 0.3871273693868489\n",
            "train crossentropy at epoch 51 loss:  0.71543292768782\n",
            "validation crossentropy at epoch 51 loss:  0.853060127004681\n",
            "Accuracy: 0.3841613084577267\n",
            "train crossentropy at epoch 52 loss:  0.7109784415407147\n",
            "validation crossentropy at epoch 52 loss:  0.8544645273945476\n",
            "Accuracy: 0.386586274370396\n",
            "train crossentropy at epoch 53 loss:  0.7063134201460605\n",
            "validation crossentropy at epoch 53 loss:  0.8436647169541993\n",
            "Accuracy: 0.38469714540246214\n",
            "train crossentropy at epoch 54 loss:  0.7020986752865983\n",
            "validation crossentropy at epoch 54 loss:  0.8415388913962665\n",
            "Accuracy: 0.38611153905407486\n",
            "train crossentropy at epoch 55 loss:  0.6965929015184478\n",
            "validation crossentropy at epoch 55 loss:  0.8391270581507843\n",
            "Accuracy: 0.3884923615359004\n",
            "train crossentropy at epoch 56 loss:  0.6911768706153623\n",
            "validation crossentropy at epoch 56 loss:  0.8369550185915607\n",
            "Accuracy: 0.3887946328875214\n",
            "train crossentropy at epoch 57 loss:  0.686734119700871\n",
            "validation crossentropy at epoch 57 loss:  0.8325763891807338\n",
            "Accuracy: 0.38843716739684664\n",
            "train crossentropy at epoch 58 loss:  0.6814142925597781\n",
            "validation crossentropy at epoch 58 loss:  0.8318682140951988\n",
            "Accuracy: 0.3881973095832201\n",
            "train crossentropy at epoch 59 loss:  0.6781855980054937\n",
            "validation crossentropy at epoch 59 loss:  0.83055842897836\n",
            "Accuracy: 0.38689355728931785\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.Adagrad(model.parameters(), lr = 0.003)\n",
        "EPOCHS = 60\n",
        "\n",
        "pad_idx = source.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "for i in range(0,EPOCHS):\n",
        "    stepLoss=[]\n",
        "    model.train()\n",
        "    for batch in train_iterator:\n",
        "        input_sentence = batch.src.to(device)\n",
        "        trg = batch.trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(input_sentence,trg[:-1])\n",
        "        out = out.reshape(-1,trg_vocab_size)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        loss = criterion(out,trg)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stepLoss.append(loss.item())\n",
        "\n",
        "\n",
        "    loss_track.append(np.mean(stepLoss))\n",
        "    print(\"train crossentropy at epoch {} loss: \".format(i),np.mean(stepLoss))\n",
        "\n",
        "    stepValidLoss=[]\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    model.eval()\n",
        "    torch.save(model.state_dict(), 'model{:02d}.pth'.format(i+1))\n",
        "    for batch  in valid_iterator:\n",
        "        input_sentence = batch.src.to(device)\n",
        "        trg = batch.trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(input_sentence,trg[:-1])\n",
        "        predictions = out.argmax(dim=-1)\n",
        "        total_correct += (predictions == trg[1:]).sum().item()\n",
        "        total_examples += trg[1:].numel()\n",
        "\n",
        "        out = out.reshape(-1,trg_vocab_size)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        loss = criterion(out,trg)\n",
        "\n",
        "        stepValidLoss.append(loss.item())\n",
        "\n",
        "    loss_validation_track.append(np.mean(stepValidLoss))\n",
        "    accuracy = total_correct / total_examples\n",
        "    print(\"validation crossentropy at epoch {} loss: \".format(i),np.mean(stepValidLoss))\n",
        "    print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mucGUPLOvlKp"
      },
      "source": [
        "## Đánh giá mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KjoU6sygBVP8"
      },
      "outputs": [],
      "source": [
        "def translate(model,sentence,srcField,targetField,srcTokenizer):\n",
        "    model.eval()\n",
        "    processed_sentence = srcField.process([srcTokenizer(sentence)]).to(device)\n",
        "    trg = [\"<sos>\"]\n",
        "    for iter in range(60):\n",
        "\n",
        "        trg_indecies = [targetField.vocab.stoi[word] for word in trg]\n",
        "        outputs = torch.Tensor(trg_indecies).unsqueeze(1).to(device)\n",
        "        outputs = model(processed_sentence,outputs)\n",
        "\n",
        "        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"<unk>\":\n",
        "            continue\n",
        "\n",
        "        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"<eos>\":\n",
        "            break\n",
        "\n",
        "        trg.append(targetField.vocab.itos[outputs.argmax(2)[-1:].item()])\n",
        "\n",
        "    return \" \".join([word for word in trg if word != \"<unk>\"][1:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMoBDKNTx_Ob"
      },
      "source": [
        "Kiểm tra độ chính xác của các mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XakaLTm540fG",
        "outputId": "7fb941ba-fc23-4877-cb36-a30878940808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 1\n",
            "Accuracy: 0.27167535242951407\n",
            "Translated:  i saw a lot of the same time it was all\n",
            "=================================================================================\n",
            "Model 2\n",
            "Accuracy: 0.30778777246939387\n",
            "Translated:  i saw a very tired because because it was done all\n",
            "=================================================================================\n",
            "Model 3\n",
            "Accuracy: 0.32508076385770596\n",
            "Translated:  i caught very tired on the same time because it was all\n",
            "=================================================================================\n",
            "Model 4\n",
            "Accuracy: 0.33409286034242486\n",
            "Translated:  i feel very tired on the concert because it was done all\n",
            "=================================================================================\n",
            "Model 5\n",
            "Accuracy: 0.342286299765808\n",
            "Translated:  i feel very tired on the concert because it was done all\n",
            "=================================================================================\n",
            "Model 6\n",
            "Accuracy: 0.3473198237555544\n",
            "Translated:  i feel very tired on the whole time did all\n",
            "=================================================================================\n",
            "Model 7\n",
            "Accuracy: 0.3514703136669156\n",
            "Translated:  i feel very tired on the whole time did all\n",
            "=================================================================================\n",
            "Model 8\n",
            "Accuracy: 0.3554212173349292\n",
            "Translated:  i feel very tired on the whole thing that has done all\n",
            "=================================================================================\n",
            "Model 9\n",
            "Accuracy: 0.35673079315320694\n",
            "Translated:  i feel very tired tired because it did all day all\n",
            "=================================================================================\n",
            "Model 10\n",
            "Accuracy: 0.3607755079175381\n",
            "Translated:  i feel very tired tired because it did all day all\n",
            "=================================================================================\n",
            "Model 11\n",
            "Accuracy: 0.36384688563989587\n",
            "Translated:  i feel very tired tired because it did all day all\n",
            "=================================================================================\n",
            "Model 12\n",
            "Accuracy: 0.36472312297331005\n",
            "Translated:  i feel very tired tired because it did all day all\n",
            "=================================================================================\n",
            "Model 13\n",
            "Accuracy: 0.36491377375430195\n",
            "Translated:  i feel so tired that the gate has done all day all\n",
            "=================================================================================\n",
            "Model 14\n",
            "Accuracy: 0.3670760648079121\n",
            "Translated:  i feel very tired that the whole thing did all\n",
            "=================================================================================\n",
            "Model 15\n",
            "Accuracy: 0.36830667809921463\n",
            "Translated:  i feel very tired that the whole time did all day all\n",
            "=================================================================================\n",
            "Model 16\n",
            "Accuracy: 0.37089389897277353\n",
            "Translated:  i feel very tired that the whole thing did all\n",
            "=================================================================================\n",
            "Model 17\n",
            "Accuracy: 0.3729813509324534\n",
            "Translated:  i feel very tired tired that has been tired for work all\n",
            "=================================================================================\n",
            "Model 18\n",
            "Accuracy: 0.37324926481583015\n",
            "Translated:  i feel very tired that the whole thing did all day all\n",
            "=================================================================================\n",
            "Model 19\n",
            "Accuracy: 0.3742690149625935\n",
            "Translated:  i feel very tired that the whole thing did all day all\n",
            "=================================================================================\n",
            "Model 20\n",
            "Accuracy: 0.37286118137181967\n",
            "Translated:  i feel very tired that the whole thing did all day all\n",
            "=================================================================================\n",
            "Model 21\n",
            "Accuracy: 0.3776969863836604\n",
            "Translated:  i feel very tired that the tired was tired all\n",
            "=================================================================================\n",
            "Model 22\n",
            "Accuracy: 0.3770166833291677\n",
            "Translated:  i feel very tired that the tired was tired for doing all\n",
            "=================================================================================\n",
            "Model 23\n",
            "Accuracy: 0.3788122717951283\n",
            "Translated:  i feel very tired that the tired was tired for work all\n",
            "=================================================================================\n",
            "Model 24\n",
            "Accuracy: 0.3781708092341444\n",
            "Translated:  i feel very tired that the whole thing did all\n",
            "=================================================================================\n",
            "Model 25\n",
            "Accuracy: 0.37747882829185914\n",
            "Translated:  i feel very tired that the tired was tired all\n",
            "=================================================================================\n",
            "Model 26\n",
            "Accuracy: 0.3802682231776822\n",
            "Translated:  i feel very tired that the whole thing did all day all\n",
            "=================================================================================\n",
            "Model 27\n",
            "Accuracy: 0.37941044552975184\n",
            "Translated:  i feel very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 28\n",
            "Accuracy: 0.37961766021975835\n",
            "Translated:  i feel very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 29\n",
            "Accuracy: 0.3806842229746552\n",
            "Translated:  i feel very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 30\n",
            "Accuracy: 0.3820575072304777\n",
            "Translated:  i feel very tired that the whole thing did all day all\n",
            "=================================================================================\n",
            "Model 31\n",
            "Accuracy: 0.3803155774395703\n",
            "Translated:  i feel very tired that the tired was tired for doing all\n",
            "=================================================================================\n",
            "Model 32\n",
            "Accuracy: 0.38401054435383\n",
            "Translated:  i feel very tired that the tired was tired that all day\n",
            "=================================================================================\n",
            "Model 33\n",
            "Accuracy: 0.3823125715850804\n",
            "Translated:  i felt very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 34\n",
            "Accuracy: 0.3847011847630474\n",
            "Translated:  i felt very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 35\n",
            "Accuracy: 0.38403804374968786\n",
            "Translated:  i felt very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 36\n",
            "Accuracy: 0.3868269520272983\n",
            "Translated:  i felt very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 37\n",
            "Accuracy: 0.3857937193351357\n",
            "Translated:  i felt very tired that the tired was tired for doing all\n",
            "=================================================================================\n",
            "Model 38\n",
            "Accuracy: 0.3850249066002491\n",
            "Translated:  i felt very tired that the tired was tired all day\n",
            "=================================================================================\n",
            "Model 39\n",
            "Accuracy: 0.38305872150644205\n",
            "Translated:  i feel very tired that the tired was tired because all\n",
            "=================================================================================\n",
            "Model 40\n",
            "Accuracy: 0.3856465452824549\n",
            "Translated:  i feel very tired that the tired was tired of doing all\n",
            "=================================================================================\n",
            "Model 41\n",
            "Accuracy: 0.38512005684934925\n",
            "Translated:  i feel very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 42\n",
            "Accuracy: 0.3871791350379545\n",
            "Translated:  i felt very tired that the tired was tired for doing all\n",
            "=================================================================================\n",
            "Model 43\n",
            "Accuracy: 0.38398669554455445\n",
            "Translated:  i feel very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 44\n",
            "Accuracy: 0.38701479894364443\n",
            "Translated:  i felt very tired that the tired was tired because all day\n",
            "=================================================================================\n",
            "Model 45\n",
            "Accuracy: 0.38684261287271643\n",
            "Translated:  i feel very tired that the whole thing did it all day\n",
            "=================================================================================\n",
            "Model 46\n",
            "Accuracy: 0.38648748570860464\n",
            "Translated:  i feel very tired that the tired was tired of doing all\n",
            "=================================================================================\n",
            "Model 47\n",
            "Accuracy: 0.38892464383904024\n",
            "Translated:  i felt very tired that the tired was tired because all\n",
            "=================================================================================\n",
            "Model 48\n",
            "Accuracy: 0.38818422693266835\n",
            "Translated:  i felt very tired that the tired was tired since all\n",
            "=================================================================================\n",
            "Model 49\n",
            "Accuracy: 0.3881892878500947\n",
            "Translated:  i felt very tired that the whole thing had been tired all\n",
            "=================================================================================\n",
            "Model 50\n",
            "Accuracy: 0.38777321215436644\n",
            "Translated:  i felt very tired that the whole thing did it all day\n",
            "=================================================================================\n",
            "Model 51\n",
            "Accuracy: 0.3876307323495802\n",
            "Translated:  i feel very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 52\n",
            "Accuracy: 0.39100346020761245\n",
            "Translated:  i felt very tired that the whole thing did all day\n",
            "=================================================================================\n",
            "Model 53\n",
            "Accuracy: 0.3888530445840814\n",
            "Translated:  i felt very tired that the tired was tired of doing all\n",
            "=================================================================================\n",
            "Model 54\n",
            "Accuracy: 0.3878745841402254\n",
            "Translated:  i feel very tired that the tired was tired all day\n",
            "=================================================================================\n",
            "Model 55\n",
            "Accuracy: 0.3893636635439506\n",
            "Translated:  i felt very tired that the tired was tired of doing all\n",
            "=================================================================================\n",
            "Model 56\n",
            "Accuracy: 0.38908146388778353\n",
            "Translated:  i felt very tired that the whole thing had been tired all\n",
            "=================================================================================\n",
            "Model 57\n",
            "Accuracy: 0.3900794343575419\n",
            "Translated:  i feel very tired that the whole thing was tired all day\n",
            "=================================================================================\n",
            "Model 58\n",
            "Accuracy: 0.39035861166500496\n",
            "Translated:  i felt very tired that the whole thing had been tired all\n",
            "=================================================================================\n",
            "Model 59\n",
            "Accuracy: 0.391380419039057\n",
            "Translated:  i felt very tired that the whole thing had been tired all day\n",
            "=================================================================================\n",
            "Model 60\n",
            "Accuracy: 0.38974792422811116\n",
            "Translated:  i felt very tired that the whole thing had been tired all\n",
            "=================================================================================\n"
          ]
        }
      ],
      "source": [
        "model_test = TranslateTransformer(\n",
        "    embedding_size,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    max_len\n",
        ").to(device)\n",
        "\n",
        "model_accuracy = []\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  model_path = '/content/model{:02d}.pth'.format(i+1)\n",
        "  model_test.load_state_dict(torch.load(model_path))\n",
        "\n",
        "  model_test.eval()\n",
        "  total_correct = 0\n",
        "  total_examples = 0\n",
        "\n",
        "  for batch in test_iterator:\n",
        "      input_sentence = batch.src.to(device)\n",
        "      trg = batch.trg.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output = model_test(input_sentence, trg[:-1])\n",
        "          predictions = output.argmax(dim=-1)\n",
        "\n",
        "      total_correct += (predictions == trg[1:]).sum().item()\n",
        "      total_examples += trg[1:].numel()\n",
        "\n",
        "  accuracy = total_correct / total_examples\n",
        "  model_accuracy.append(accuracy)\n",
        "  print(f\"Model {i+1}\")\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  print(\"Translated: \",\n",
        "        translate(model_test,transform_text('Tôi cảm thấy rất mệt mỏi vì đã làm việc suốt cả ngày.', eng=False), source, target, tokenize_en))\n",
        "  print('=================================================================================')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixOmiH5px5Ob"
      },
      "source": [
        "Trực quan độ chính xác của các mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ISC_FVOwsCOm",
        "outputId": "45cd496f-ad59-434f-a2c0-8e0852195fd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x=list(range(1, EPOCHS + 1)), y=model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm_78Y4hsbkJ",
        "outputId": "37998d30-075b-4b17-e04d-8090a8d2871a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best model is model59.pth\n",
            "Accuracy = 0.3914\n"
          ]
        }
      ],
      "source": [
        "print('The best model is model{:02d}.pth\\nAccuracy = {:.4f}'.format(model_accuracy.index(max(model_accuracy)) + 1, max(model_accuracy)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
